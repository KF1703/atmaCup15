{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport random\nimport sys\n# sys.path.append('../input/iterativestratification')\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nfrom datetime import date, datetime\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import KFold , StratifiedKFold, GroupKFold\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, MultiLabelBinarizer\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as op_lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\nfrom gensim.models import word2vec\nfrom scipy.spatial.distance import pdist\nfrom scipy.spatial.distance import squareform\n# from tabpfn import TabPFNClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:42:17.582583Z","iopub.execute_input":"2023-07-23T08:42:17.583263Z","iopub.status.idle":"2023-07-23T08:42:23.267672Z","shell.execute_reply.started":"2023-07-23T08:42:17.583224Z","shell.execute_reply":"2023-07-23T08:42:23.266375Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install ptitprince\n# import ptitprince as pt \n# from ptitprince import RainCloud","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:42:23.269848Z","iopub.execute_input":"2023-07-23T08:42:23.270204Z","iopub.status.idle":"2023-07-23T08:42:23.275644Z","shell.execute_reply.started":"2023-07-23T08:42:23.270173Z","shell.execute_reply":"2023-07-23T08:42:23.274200Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\nseed_everything(0)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:42:23.277414Z","iopub.execute_input":"2023-07-23T08:42:23.277852Z","iopub.status.idle":"2023-07-23T08:42:23.290567Z","shell.execute_reply.started":"2023-07-23T08:42:23.277811Z","shell.execute_reply":"2023-07-23T08:42:23.289167Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def create_kfold_seen(df):\n    sgk = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    df[\"kfold\"] = -1\n    for i, (tr_id, va_id) in enumerate(sgk.split(df, df[\"user_id\"].values)):\n        df.loc[va_id, \"kfold\"] = int(i)   \n    return df\n\ndef create_kfold_unseen(df):\n    sgk = GroupKFold(n_splits=5)\n    df[\"kfold\"] = -1\n    for i, (tr_id, va_id) in enumerate(sgk.split(df, df[\"user_id\"], df[\"user_id\"])):\n        df.loc[va_id, \"kfold\"] = int(i)   \n    return df\n\n\ndef stratified_and_group_kfold_split(train_df):\n    # https://www.guruguru.science/competitions/21/discussions/45ffc8a1-e37c-4b95-aac4-c4e338aa6a9b/\n\n    # 20%のユーザを抽出\n    n_user = train_df[\"user_id\"].nunique()\n    unseen_users = random.sample(sorted(train_df[\"user_id\"].unique()), k=n_user // 5)\n    train_df[\"unseen_user\"] = train_df[\"user_id\"].isin(unseen_users)\n    unseen_df = train_df[train_df[\"unseen_user\"]].reset_index(drop=True)\n    train_df = train_df[~train_df[\"unseen_user\"]].reset_index(drop=True)\n\n    # train_dfの80%をStratifiedKFoldで分割\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    for fold_id, (_, valid_idx) in enumerate(skf.split(train_df, train_df[\"user_id\"])):\n        train_df.loc[valid_idx, \"fold\"] = fold_id\n\n    # 20%をGroupKFoldで分割\n    gkf = GroupKFold(n_splits=5)\n    unseen_df[\"fold\"] = -1\n    for fold_id, (_, valid_idx) in enumerate(gkf.split(unseen_df, unseen_df[\"user_id\"], unseen_df[\"user_id\"])):\n        unseen_df.loc[valid_idx, \"fold\"] = fold_id\n\n    # concat\n    train_df = pd.concat([train_df, unseen_df], axis=0).reset_index(drop=True)\n    train_df.drop(columns=[\"unseen_user\"], inplace=True)\n    return train_df","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:42:23.294088Z","iopub.execute_input":"2023-07-23T08:42:23.294903Z","iopub.status.idle":"2023-07-23T08:42:23.310532Z","shell.execute_reply.started":"2023-07-23T08:42:23.294856Z","shell.execute_reply":"2023-07-23T08:42:23.309163Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"###Anime2Vec### \ndef add_w2v_features(train_df, test_df, consider_score=True):\n    anime_ids = train_df['anime_id'].unique().tolist()\n    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_df.groupby('user_id')['anime_id']}\n\n    # スコアを考慮する場合\n    # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n    if consider_score:\n        title_sentence_list = []\n        for user_id, user_df in train_df.groupby('user_id'):\n            user_title_sentence_list = []\n            for anime_id, anime_score in user_df[['anime_id', 'score']].values:\n                for i in range(anime_score):\n                    user_title_sentence_list.append(anime_id)\n            title_sentence_list.append(user_title_sentence_list)\n    # スコアを考慮しない場合\n    # タイトルをそのままリストに追加する\n    else:\n        title_sentence_list = train_df.groupby('user_id')['anime_id'].apply(list).tolist()\n\n    # ユーザごとにshuffleしたリストを作成\n    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n\n    # 元のリストとshuffleしたリストを合わせる\n    train_sentence_list = title_sentence_list + shuffled_sentence_list\n\n    # word2vecのパラメータ\n    vector_size = 64\n    w2v_params = {\n        \"vector_size\": vector_size,  ## <= 変更点\n        \"seed\": 0,\n        \"min_count\": 1,\n        \"workers\": 1\n    }\n\n    # word2vecのモデル学習\n    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n\n    # ユーザーごとの特徴ベクトルと対応するユーザーID\n    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n\n    # アイテムごとの特徴ベクトルと対応するアイテムID\n    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n\n    # データフレームを作成\n    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n\n    # データフレームのカラム名をリネーム\n    user_factors_df.columns = [\"user_id\"] + [f\"anime_vec_user_factor_{i}\" for i in range(vector_size)]\n    item_factors_df.columns = [\"anime_id\"] + [f\"anime_vec_anime_factor_{i}\" for i in range(vector_size)]\n\n    train_df = train_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n    train_df = train_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n    \n    test_df = test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n    test_df = test_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n    \n    return train_df, test_df\n\ndef add_w2v_features_without_score(train_df, test_df, train_test_df):\n\n    anime_ids = train_test_df['anime_id'].unique().tolist()\n    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_test_df.groupby('user_id')['anime_id']}\n\n    title_sentence_list = train_test_df.groupby('user_id')['anime_id'].apply(list).tolist()\n\n    # ユーザごとにshuffleしたリストを作成\n    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n\n    # 元のリストとshuffleしたリストを合わせる\n    train_sentence_list = title_sentence_list + shuffled_sentence_list\n\n    # word2vecのパラメータ\n    vector_size = 64\n    w2v_params = {\n        \"vector_size\": vector_size,  ## <= 変更点\n        \"seed\": 0,\n        \"min_count\": 1,\n        \"workers\": 1\n    }\n\n    # word2vecのモデル学習\n    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n\n    # ユーザーごとの特徴ベクトルと対応するユーザーID\n    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n\n    # アイテムごとの特徴ベクトルと対応するアイテムID\n    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n\n    # データフレームを作成\n    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n\n    # データフレームのカラム名をリネーム\n    user_factors_df.columns = [\"user_id\"] + [f\"anime_vec_wo_score_user_factor_{i}\" for i in range(vector_size)]\n    item_factors_df.columns = [\"anime_id\"] + [f\"anime_vec_wo_score_item_factor_{i}\" for i in range(vector_size)]\n    \n    train_df = train_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n#     train_df = train_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n    \n    test_df = test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n#     test_df = test_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n\n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:42:23.312208Z","iopub.execute_input":"2023-07-23T08:42:23.312553Z","iopub.status.idle":"2023-07-23T08:42:23.339161Z","shell.execute_reply.started":"2023-07-23T08:42:23.312524Z","shell.execute_reply":"2023-07-23T08:42:23.337840Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"###User2Vec### \ndef add_w2v_user_features(train_df, test_df, consider_score=True):\n    user_ids = train_df['user_id'].unique().tolist()\n    user_anime_list_dict = {anime_id: user_ids.tolist() for anime_id, user_ids in train_df.groupby('anime_id')['user_id']}\n\n    # スコアを考慮する場合\n    # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n    if consider_score:\n        title_sentence_list = []\n        for user_id, user_df in train_df.groupby('anime_id'):\n            user_title_sentence_list = []\n            for anime_id, anime_score in user_df[['user_id', 'score']].values:\n                for i in range(anime_score):\n                    user_title_sentence_list.append(anime_id)\n            title_sentence_list.append(user_title_sentence_list)\n    # スコアを考慮しない場合\n    # タイトルをそのままリストに追加する\n    else:\n        title_sentence_list = train_df.groupby('anime_id')['user_id'].apply(list).tolist()\n\n    # ユーザごとにshuffleしたリストを作成\n    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n\n    # 元のリストとshuffleしたリストを合わせる\n    train_sentence_list = title_sentence_list + shuffled_sentence_list\n\n    # word2vecのパラメータ\n    vector_size = 64\n    w2v_params = {\n        \"vector_size\": vector_size,  ## <= 変更点\n        \"seed\": 0,\n        \"min_count\": 1,\n        \"workers\": 1\n    }\n\n    # word2vecのモデル学習\n    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n\n    # アニメごとの特徴ベクトルと対応するアニメID\n    user_factors = {anime_id: np.mean([model.wv[user_id] for user_id in user_anime_list], axis=0) for anime_id, user_anime_list in user_anime_list_dict.items()}\n\n    # アイテムごとの特徴ベクトルと対応するアイテムID\n    item_factors = {uid: model.wv[uid] for uid in user_ids}\n\n    # データフレームを作成\n    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n\n    # データフレームのカラム名をリネーム\n    user_factors_df.columns = [\"anime_id\"] + [f\"user_vec_anime_factor_{i}\" for i in range(vector_size)]\n    item_factors_df.columns = [\"user_id\"] + [f\"user_vec_user_factor_{i}\" for i in range(vector_size)]\n\n    train_df = train_df.merge(user_factors_df, on=\"anime_id\", how=\"left\")\n    train_df = train_df.merge(item_factors_df, on=\"user_id\", how=\"left\")\n    \n    test_df = test_df.merge(user_factors_df, on=\"anime_id\", how=\"left\")\n    test_df = test_df.merge(item_factors_df, on=\"user_id\", how=\"left\")\n    \n    return train_df, test_df\n\ndef add_w2v_user_features_without_score(train_df, test_df, train_test_df):\n\n    user_ids = train_test_df['user_id'].unique().tolist()\n    user_anime_list_dict = {anime_id: user_ids.tolist() for anime_id, user_ids in train_test_df.groupby('anime_id')['user_id']}\n\n    title_sentence_list = train_test_df.groupby('anime_id')['user_id'].apply(list).tolist()\n\n    # ユーザごとにshuffleしたリストを作成\n    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n\n    # 元のリストとshuffleしたリストを合わせる\n    train_sentence_list = title_sentence_list + shuffled_sentence_list\n\n    # word2vecのパラメータ\n    vector_size = 64\n    w2v_params = {\n        \"vector_size\": vector_size,  ## <= 変更点\n        \"seed\": 0,\n        \"min_count\": 1,\n        \"workers\": 1\n    }\n\n    # word2vecのモデル学習\n    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n\n    # ユーザーごとの特徴ベクトルと対応するユーザーID\n    user_factors = {anime_id: np.mean([model.wv[user_id] for user_id in user_anime_list], axis=0) for anime_id, user_anime_list in user_anime_list_dict.items()}\n\n    # アイテムごとの特徴ベクトルと対応するアイテムID\n    item_factors = {aid: model.wv[aid] for aid in user_ids}\n\n    # データフレームを作成\n    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n\n    # データフレームのカラム名をリネーム\n    user_factors_df.columns = [\"anime_id\"] + [f\"user_vec_wo_score_anime_factor_{i}\" for i in range(vector_size)]\n    item_factors_df.columns = [\"user_id\"] + [f\"user_vec_wo_score_user_factor_{i}\" for i in range(vector_size)]\n    \n    train_df = train_df.merge(user_factors_df, on=\"anime_id\", how=\"left\")\n#     train_df = train_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n    \n    test_df = test_df.merge(user_factors_df, on=\"anime_id\", how=\"left\")\n#     test_df = test_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n\n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:42:23.340716Z","iopub.execute_input":"2023-07-23T08:42:23.341239Z","iopub.status.idle":"2023-07-23T08:42:23.368186Z","shell.execute_reply.started":"2023-07-23T08:42:23.341205Z","shell.execute_reply":"2023-07-23T08:42:23.366956Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"###Etc2Vec### \ndef add_w2v_genres_features(train_df, test_df, train_test_df, col,consider_score=True):\n    # スコアを考慮する場合\n    # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n    if consider_score:\n        genres_ids = train_df[col].unique().tolist()\n        user_anime_list_dict = {user_id: genres_ids.tolist() for user_id, genres_ids in train_df.groupby('user_id')[col]}\n        title_sentence_list = []\n        for user_id, user_df in train_df.groupby('user_id'):\n            user_title_sentence_list = []\n            for anime_id, anime_score in user_df[[col, 'score']].values:\n                for i in range(anime_score):\n                    user_title_sentence_list.append(anime_id)\n            title_sentence_list.append(user_title_sentence_list)\n    # スコアを考慮しない場合\n    # タイトルをそのままリストに追加する\n    else:\n        genres_ids = train_test_df[col].unique().tolist()\n        user_anime_list_dict = {user_id: genres_ids.tolist() for user_id, genres_ids in train_test_df.groupby('user_id')[col]}\n        title_sentence_list = train_test_df.groupby('user_id')[col].apply(list).tolist()\n\n    # ユーザごとにshuffleしたリストを作成\n    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n\n    # 元のリストとshuffleしたリストを合わせる\n    train_sentence_list = title_sentence_list + shuffled_sentence_list\n\n    # word2vecのパラメータ\n    vector_size = 64\n    w2v_params = {\n        \"vector_size\": vector_size,  ## <= 変更点\n        \"seed\": 0,\n        \"min_count\": 1,\n        \"workers\": 1\n    }\n\n    # word2vecのモデル学習\n    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n\n    # userごとの特徴ベクトルと対応するuserID\n    user_factors = {user_id: np.mean([model.wv[genres] for genres in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n\n    # データフレームを作成\n    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n    \n    # データフレームのカラム名をリネーム\n    if consider_score:\n        user_factors_df.columns = [\"user_id\"] + [f\"user_vec_{col}_factor_{i}\" for i in range(vector_size)]\n    else:\n        user_factors_df.columns = [\"user_id\"] + [f\"user_vec_wo_score_{col}_factor_{i}\" for i in range(vector_size)]\n\n    train_df = train_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n    test_df = test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n    \n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:42:23.371908Z","iopub.execute_input":"2023-07-23T08:42:23.372330Z","iopub.status.idle":"2023-07-23T08:42:23.391279Z","shell.execute_reply.started":"2023-07-23T08:42:23.372297Z","shell.execute_reply":"2023-07-23T08:42:23.390069Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def onehot(df, col):\n    list_srs = df[col].map(lambda x: x.split(\", \")).tolist()\n    count = df[col].map(lambda x: len(x.split(\", \")))\n    mlb = MultiLabelBinarizer()\n    ohe_srs = mlb.fit_transform(list_srs)\n    if col == \"genres\" or col == \"licensors\":\n        col_df = pd.DataFrame(ohe_srs, columns=[f\"ohe_{col}_{name}\" for name in mlb.classes_])\n    else:\n        components_num = 15\n        svd = TruncatedSVD(n_components=components_num, random_state=0)\n        svd_arr = svd.fit_transform(ohe_srs)\n        col_df = pd.DataFrame(svd_arr, columns=[f\"svd_{col}_{ix}\" for ix in range(components_num)])\n    \n    col_df[f\"{col}_count_num\"] = count\n    \n    return col_df\n\ndef duration_transform(d):\n    if d == \"Unknown\":\n        return np.nan\n    elif \"min\" not in d:\n        return int(d[0])*60\n    elif \"hr\" in d:\n        d = d.split(\".\")\n        count = int(d[0].replace(\"hr\",\"\"))*60 + int(d[1].replace(\"min\",\"\"))\n        return count\n    else:\n        d = d.split(\".\")\n        count = int(d[0].replace(\"min\",\"\"))\n        return count\n\ndef anime_feature_create(df):\n    for c in [\"genres\", \"producers\", \"studios\", \"licensors\"]:\n        _df = onehot(df, c)\n        df = pd.concat([df, _df], axis=1)\n        \n    df[\"genres\"] = df[\"genres\"].map(lambda x: \", \".join(sorted(x.split(\", \"))))\n    \n    le = LabelEncoder()\n    df[\"type_and_source\"] = df[\"type\"] + \" and \" + df[\"source\"]\n    df[\"type\"] = le.fit_transform(df[\"type\"])\n    df[\"source\"] = le.fit_transform(df[\"source\"])\n    df[\"rating\"] = le.fit_transform(df[\"rating\"])\n    df[\"type_and_source_le\"] = le.fit_transform(df[\"type_and_source\"])\n    \n    df[\"is_contains_ep\"] = df[\"duration\"].str.contains(\"per\")*1\n    \n    for c in [\"watching\", \"completed\", \"on_hold\", \"dropped\", \"plan_to_watch\"]:\n        df[f\"{c}_per_member\"] = df[c] / df[\"members\"]\n    \n    df[\"episodes\"] = df[\"episodes\"].replace({\"Unknown\": np.nan})\n    df[\"episodes\"] = pd.to_numeric(df[\"episodes\"], errors='coerce')\n    \n    df[\"duration\"] = df[\"duration\"].apply(lambda x: duration_transform(x))\n    df[\"total_duration\"] = df[\"duration\"] * df[\"episodes\"]\n    \n    return df\n\ndef user_similarity(train_df, train_test_df):\n    mlb = MultiLabelBinarizer()\n    user_anime_list = pd.DataFrame(train_test_df.groupby(\"user_id\")[\"anime_id\"].apply(list).reset_index())\n    matrix = mlb.fit_transform(user_anime_list[\"anime_id\"])\n    _matrix = pdist(matrix, 'cosine')\n    _matrix = 1 - _matrix\n    _matrix = squareform(_matrix)\n    components_num = 32\n    svd = TruncatedSVD(n_components=components_num, random_state=0)\n    svd_arr = svd.fit_transform(matrix)\n    _svd_arr = svd.fit_transform(_matrix)\n    col_df = pd.DataFrame(svd_arr, columns=[f\"svd_user_anime_looked_{ix}\" for ix in range(components_num)])\n    _col_df = pd.DataFrame(_svd_arr, columns=[f\"svd_user_anime_looked_similarity_{ix}\" for ix in range(components_num)])\n    _user_anime_list1 = pd.concat([user_anime_list[\"user_id\"], col_df, _col_df], axis=1)\n    \n    user_anime_list = pd.DataFrame(train_df.groupby(\"user_id\")[\"anime_id\"].apply(list).reset_index())\n    matrix = mlb.fit_transform(user_anime_list[\"anime_id\"])\n    anime_list = sorted(train_df[\"anime_id\"].unique())\n    user_list = user_anime_list[\"user_id\"].values\n    for i in tqdm(range(matrix.shape[0])):\n        idxs = np.where(matrix[i]==1)[0]\n        user = user_list[i]\n        _df = train_df[train_df[\"user_id\"]==user]\n        for idx in idxs:\n            anime = anime_list[idx]\n            score = _df[_df[\"anime_id\"]==anime][\"score\"].values\n            matrix[i,idx] = score\n    _matrix = pdist(matrix, 'cosine')\n    _matrix = 1 - _matrix\n    _matrix = squareform(_matrix)\n    svd_arr = svd.fit_transform(matrix)\n    _svd_arr = svd.fit_transform(_matrix)\n    col_df = pd.DataFrame(svd_arr, columns=[f\"svd_user_anime_score_{ix}\" for ix in range(components_num)])\n    _col_df = pd.DataFrame(_svd_arr, columns=[f\"svd_user_anime_score_similarity_{ix}\" for ix in range(components_num)])\n    _user_anime_list2 = pd.concat([user_anime_list[\"user_id\"], col_df, _col_df], axis=1)\n    \n    return _user_anime_list1, _user_anime_list2\n\ndef anime_similarity(train_df, train_test_df):\n    mlb = MultiLabelBinarizer()\n    user_anime_list = pd.DataFrame(train_test_df.groupby(\"anime_id\")[\"user_id\"].apply(list).reset_index())\n    matrix = mlb.fit_transform(user_anime_list[\"user_id\"])\n    _matrix = pdist(matrix, 'cosine')\n    _matrix = 1 - _matrix\n    _matrix = squareform(_matrix)\n    components_num = 32\n    svd = TruncatedSVD(n_components=components_num, random_state=0)\n    svd_arr = svd.fit_transform(matrix)\n    _svd_arr = svd.fit_transform(_matrix)\n    col_df = pd.DataFrame(svd_arr, columns=[f\"svd_anime_user_looked_{ix}\" for ix in range(components_num)])\n    _col_df = pd.DataFrame(_svd_arr, columns=[f\"svd_anime_user_looked_similarity_{ix}\" for ix in range(components_num)])\n    _user_anime_list1 = pd.concat([user_anime_list[\"anime_id\"], col_df, _col_df], axis=1)\n    \n    user_anime_list = pd.DataFrame(train_df.groupby(\"anime_id\")[\"user_id\"].apply(list).reset_index())\n    matrix = mlb.fit_transform(user_anime_list[\"user_id\"])\n    user_list = sorted(train_df[\"user_id\"].unique())\n    anime_list = user_anime_list[\"anime_id\"].values\n    for i in tqdm(range(matrix.shape[0])):\n        idxs = np.where(matrix[i]==1)[0]\n        anime = anime_list[i]\n        _df = train_df[train_df[\"anime_id\"]==anime]\n        for idx in idxs:\n            user = user_list[idx]\n            score = _df[_df[\"user_id\"]==user][\"score\"].values\n            matrix[i,idx] = score\n    _matrix = pdist(matrix, 'cosine')\n    _matrix = 1 - _matrix\n    _matrix = squareform(_matrix)\n    svd_arr = svd.fit_transform(matrix)\n    _svd_arr = svd.fit_transform(_matrix)\n    col_df = pd.DataFrame(svd_arr, columns=[f\"svd_anime_user_score_{ix}\" for ix in range(components_num)])\n    _col_df = pd.DataFrame(_svd_arr, columns=[f\"svd_anime_user_score_similarity_{ix}\" for ix in range(components_num)])\n    _user_anime_list2 = pd.concat([user_anime_list[\"anime_id\"], col_df, _col_df], axis=1)\n    \n    return _user_anime_list1, _user_anime_list2\n    \n\ndef feature_create(train_df, test_df, train_test_df):\n    ###ANIME###\n    _df = train_test_df.groupby(\"anime_id\")[\"user_id\"].agg([\"count\"]).add_prefix(\"user_id_\")\n    train_df = train_df.merge(_df, how=\"left\", on=\"anime_id\")\n    test_df = test_df.merge(_df, how=\"left\", on=\"anime_id\")\n    train_test_df = train_test_df.merge(_df, how=\"left\", on=\"anime_id\")\n    \n    ###USER###\n#     counts_df = pd.DataFrame(tr_df[\"user_id\"].value_counts(sort=True)).reset_index()\n#     small_num_users = counts_df[counts_df[\"user_id\"] < 50][\"index\"].values\n#     _df = tr_df.groupby(\"user_id\")[\"score\"].agg([\"mean\", \"var\"]).add_prefix(\"user_score_\").reset_index()\n#     te_df = te_df.merge(_df, how=\"left\", on=\"user_id\")\n#     _df.loc[_df[\"user_id\"].isin(small_num_users)] = np.nan\n#     tr_df = tr_df.merge(_df, how=\"left\", on=\"user_id\")\n    _df = train_test_df.groupby(\"user_id\")[\"anime_id\"].agg([\"count\"]).add_prefix(\"looked_anime_\")\n    train_df = train_df.merge(_df, how=\"left\", on=\"user_id\")\n    test_df = test_df.merge(_df, how=\"left\", on=\"user_id\")\n    \n    _df = pd.DataFrame()\n    for c in [\"members\", \"watching\", \"completed\", \"on_hold\", \"dropped\", \"plan_to_watch\"]:\n        if c == \"members\":\n            _df = pd.concat([\n                _df,\n                train_df.groupby(\"user_id\")[c].agg([\"mean\", \"median\", \"var\"]).add_prefix(f\"per_user_{c}_\"),\n            ], axis=1)\n        else:\n            _df = pd.concat([\n                _df,\n                train_df.groupby(\"user_id\")[c].agg([\"mean\", \"median\", \"var\"]).add_prefix(f\"per_user_{c}_\"),\n                train_df.groupby(\"user_id\")[f\"{c}_per_member\"].agg([\"mean\", \"median\", \"var\"]).add_prefix(f\"per_user_{c}_per_member_\"),\n            ], axis=1)\n    \n    train_df = train_df.merge(_df, how=\"left\", on=\"user_id\")\n    test_df = test_df.merge(_df, how=\"left\", on=\"user_id\")\n    \n    _df = train_test_df.groupby(\"user_id\")[\"user_id_count\"].agg([\"mean\", \"median\", \"var\"]).add_prefix(\"user_id_count_\")\n    train_df = train_df.merge(_df, how=\"left\", on=\"user_id\")\n    test_df = test_df.merge(_df, how=\"left\", on=\"user_id\")\n    \n#     _df = train_df.groupby(\"user_id\")[\"score\"].agg([\"var\", \"median\", \"mean\"]).add_prefix(\"user_score_\")\n#     train_df = train_df.merge(_df, how=\"left\", on=\"user_id\")\n#     test_df = test_df.merge(_df, how=\"left\", on=\"user_id\")\n    \n    ohe_genres_c = [c for c in train_test_df.columns if \"ohe_genres\" in c]\n    _df = train_test_df.groupby(\"user_id\")[ohe_genres_c].mean().add_prefix(\"user_mean_\")\n    train_df = train_df.merge(_df, how=\"left\", on=\"user_id\")\n    test_df = test_df.merge(_df, how=\"left\", on=\"user_id\")\n    \n\n    _df1, _df2 = user_similarity(train_df, train_test_df)    \n    train_df = train_df.merge(_df1, how=\"left\", on=\"user_id\")\n    test_df = test_df.merge(_df1, how=\"left\", on=\"user_id\")\n    train_df = train_df.merge(_df2, how=\"left\", on=\"user_id\")\n    test_df = test_df.merge(_df2, how=\"left\", on=\"user_id\")\n    _df1, _df2 = anime_similarity(train_df, train_test_df)\n    train_df = train_df.merge(_df1, how=\"left\", on=\"anime_id\")\n    test_df = test_df.merge(_df1, how=\"left\", on=\"anime_id\")\n    train_df = train_df.merge(_df2, how=\"left\", on=\"anime_id\")\n    test_df = test_df.merge(_df2, how=\"left\", on=\"anime_id\")\n#     train_df = train_df.drop(ohe_genres_c, axis=1)\n#     test_df = test_df.drop(ohe_genres_c, axis=1)\n    \n#     _df = train_df.groupby(\"user_id\")[\"duration\"].agg([\"mean\", \"median\", \"max\", \"min\"]).add_prefix(\"user_anime_duration_\")\n#     train_df = train_df.merge(_df, how=\"left\", on=\"user_id\")\n#     test_df = test_df.merge(_df, how=\"left\", on=\"user_id\")\n    \n#     _df = train_df.groupby(\"user_id\")[\"total_duration\"].agg([\"mean\", \"median\", \"max\", \"min\"]).add_prefix(\"user_anime_total_duration_\")\n#     train_df = train_df.merge(_df, how=\"left\", on=\"user_id\")\n#     test_df = test_df.merge(_df, how=\"left\", on=\"user_id\")\n    \n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:47:08.071870Z","iopub.execute_input":"2023-07-23T08:47:08.072312Z","iopub.status.idle":"2023-07-23T08:47:08.133148Z","shell.execute_reply.started":"2023-07-23T08:47:08.072275Z","shell.execute_reply":"2023-07-23T08:47:08.132153Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/atmacup-15dataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/atmacup-15dataset/test.csv\")\nanime_df = pd.read_csv(\"/kaggle/input/atmacup-15dataset/anime.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:47:08.840692Z","iopub.execute_input":"2023-07-23T08:47:08.841126Z","iopub.status.idle":"2023-07-23T08:47:09.029302Z","shell.execute_reply.started":"2023-07-23T08:47:08.841086Z","shell.execute_reply":"2023-07-23T08:47:09.028035Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Seen","metadata":{}},{"cell_type":"code","source":"print(\"anime_features_create\")\n_anime_df = anime_feature_create(anime_df)\ntrain_df = train_df.merge(_anime_df, how=\"left\", on=\"anime_id\")\ntest_df = test_df.merge(_anime_df, how=\"left\", on=\"anime_id\")\nprint(\"anime_vec_create\")\ntrain_test_df = pd.concat([train_df, test_df]).reset_index(drop=True)\ntrain_df, test_df = add_w2v_features_without_score(train_df, test_df, train_test_df)\ntrain_df, test_df = add_w2v_features(train_df, test_df, consider_score=True)\nprint(\"user_vec_create\")\ntrain_df, test_df = add_w2v_user_features(train_df, test_df, consider_score=True)\ntrain_df, test_df = add_w2v_user_features_without_score(train_df, test_df, train_test_df)\n# print(\"genres_vec_create\")\n# train_df, test_df = add_w2v_genres_features(train_df, test_df, train_test_df, consider_score=True)\n# train_df, test_df = add_w2v_genres_features(train_df, test_df, train_test_df, consider_score=False)\n# print(\"type_source_vec_create\")\n# train_df, test_df = add_w2v_genres_features(train_df, test_df, train_test_df, \"type_and_source\" ,consider_score=True)\n# train_df, test_df = add_w2v_genres_features(train_df, test_df, train_test_df, \"type_and_source\" ,consider_score=False)\nprint(\"features_create\")\ntrain_test_df = pd.concat([train_df, test_df]).reset_index(drop=True)\ntrain_df, test_df = feature_create(train_df, test_df, train_test_df)\n\ntrain_df = train_df.drop([\"type_and_source\"], axis=1)\ntest_df = test_df.drop([\"type_and_source\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:47:09.602194Z","iopub.execute_input":"2023-07-23T08:47:09.602617Z","iopub.status.idle":"2023-07-23T08:52:30.018314Z","shell.execute_reply.started":"2023-07-23T08:47:09.602582Z","shell.execute_reply":"2023-07-23T08:52:30.016702Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"anime_features_create\nanime_vec_create\nuser_vec_create\nfeatures_create\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1794 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"338a3d9554d949c5a8e232e7d757f58c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1954 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9681d5c7dc43a393f4533dee8d695f"}},"metadata":{}}]},{"cell_type":"code","source":"test_seen_df = test_df[test_df[\"user_id\"].isin(train_df[\"user_id\"].unique())].reset_index(drop=True)\ntest_unseen_df = test_df[~test_df[\"user_id\"].isin(train_df[\"user_id\"].unique())].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:52:30.020568Z","iopub.execute_input":"2023-07-23T08:52:30.020934Z","iopub.status.idle":"2023-07-23T08:52:30.765325Z","shell.execute_reply.started":"2023-07-23T08:52:30.020893Z","shell.execute_reply":"2023-07-23T08:52:30.764061Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv(\"train_df.csv\", index=False)\ntest_df.to_csv(\"test_df.csv\", index=False)\n# test_seen_df.to_csv(\"test_seen_df.csv\", index=False)\n# test_unseen_df.to_csv(\"test_unseen_df.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:52:30.766643Z","iopub.execute_input":"2023-07-23T08:52:30.766967Z","iopub.status.idle":"2023-07-23T08:58:41.406241Z","shell.execute_reply.started":"2023-07-23T08:52:30.766939Z","shell.execute_reply":"2023-07-23T08:58:41.404803Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# UnSeen","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(12, 6))\n# RainCloud(data=train_df, x=\"rating\", y=\"score\", ax=ax)\n\n# fig.tight_layout()\n# ax.grid()","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:45:41.531109Z","iopub.status.idle":"2023-07-23T08:45:41.531682Z","shell.execute_reply.started":"2023-07-23T08:45:41.531492Z","shell.execute_reply":"2023-07-23T08:45:41.531511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlb = MultiLabelBinarizer()\nuser_anime_list = pd.DataFrame(train_df.groupby(\"user_id\")[\"anime_id\"].apply(list).reset_index())\nmatrix = mlb.fit_transform(user_anime_list[\"anime_id\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:45:41.532849Z","iopub.status.idle":"2023-07-23T08:45:41.533436Z","shell.execute_reply.started":"2023-07-23T08:45:41.533228Z","shell.execute_reply":"2023-07-23T08:45:41.533249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.spatial.distance import pdist\nfrom scipy.spatial.distance import squareform\n# d = pdist(matrix, 'cosine')\n# d = 1 - d\n# d = squareform(d)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T08:45:41.534722Z","iopub.status.idle":"2023-07-23T08:45:41.535165Z","shell.execute_reply.started":"2023-07-23T08:45:41.534934Z","shell.execute_reply":"2023-07-23T08:45:41.534953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}